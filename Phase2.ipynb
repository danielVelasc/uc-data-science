{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Multi-Factor Prediction of Mental Illness Incidence Rates\n",
        "**Quinn Bischoff, Eric Matteucci, Rajat Singh, Daniel Velasco**\n",
        "\n",
        "# Phase II\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Behavioural and emotional well-being is integral to the development of societies around the world. However, the rates of incidence of mental health disorders are on the rise in some places around the globe, while others are declining. Our goal is to predict these incidence rates using linear regression and deep learning methods on a rich data set.\n",
        "\n",
        "Using a combination of data sets that include news headlines, financial indicators, and population distributions and indices, to generate a prediction of incidence of disorders such as depression or anxiety, and deaths by mental health. To this end, news headlines that originate from a given country will be preprocessed using natural language processing (NLP)â€”specifically, sentiment analysis. The score, in conjunction with the aforementioned datasets, will be used to generate a linear regression model and a neural network. The ultimate goal of this project is to determine whether news headline sentiments from a country are accurate at predicting the mental health disorder rate of the country.\n",
        "\n",
        "## Phase II Goal\n",
        "The aim of this phase is to ...\n",
        "\n",
        "[X] methods will be employed:\n",
        "1. First\n",
        "2. ...\n",
        "X. Last\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from functools import reduce\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ridge\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom textblob import TextBlob\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as grid\nimport nltk\nimport numpy as np\nimport os\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# column names\n",
        "COL_COUNTRY \u003d \u0027country\u0027\n",
        "COL_GDP \u003d \u0027GDP\u0027\n",
        "COL_HDI \u003d \u0027HDI\u0027\n",
        "COL_POLARITY \u003d \u0027polarity\u0027\n",
        "COL_POPULATION_DENSITY \u003d \u0027population density\u0027\n",
        "COL_SUBJECTIVITY \u003d \u0027subjectivity\u0027\n",
        "COL_UNEMPLOYMENT \u003d \u0027unemployment rate\u0027\n",
        "COL_URBAN_DENSITY \u003d \u0027urban density (%)\u0027\n",
        "COL_YEAR \u003d \u0027year\u0027\n",
        "COL_NEWS_TEMPLATE \u003d \u0027news{}\u0027\n",
        "\n",
        "TARGETS \u003d [\n",
        "    \u0027Bipolar disorder (%)\u0027,\n",
        "    \u0027Eating disorders (%)\u0027,\n",
        "    \u0027Anxiety disorders (%)\u0027,\n",
        "    \u0027Drug use disorders (%)\u0027,\n",
        "    \u0027Depression (%)\u0027,\n",
        "    \u0027Alcohol use disorders (%)\u0027,\n",
        "]\n",
        "\n",
        "FEATURES \u003d [\n",
        "    COL_GDP,\n",
        "    COL_HDI,\n",
        "    COL_POLARITY,\n",
        "    COL_POPULATION_DENSITY,\n",
        "    COL_SUBJECTIVITY,\n",
        "    COL_UNEMPLOYMENT,\n",
        "    COL_URBAN_DENSITY,\n",
        "]\n",
        "\n",
        "# directories and filenames\n",
        "DIR_DATA \u003d \u0027data\u0027\n",
        "DIR_FINANCIAL \u003d os.path.join(DIR_DATA, \u0027financial\u0027)\n",
        "DIR_HDI \u003d os.path.join(DIR_DATA, \u0027human_development_index\u0027)\n",
        "DIR_MENTAL_HEALTH \u003d os.path.join(DIR_DATA, \u0027mental_health\u0027)\n",
        "DIR_NEWS \u003d os.path.join(DIR_DATA, \u0027news\u0027)\n",
        "DIR_POPULATION \u003d os.path.join(DIR_DATA, \u0027population\u0027)\n",
        "\n",
        "FILENAME_DISORDERS \u003d \u0027prevalence-by-mental-and-substance-use-disorder.csv\u0027\n",
        "FILENAME_GDP \u003d \u0027wrldbnk_gdp.csv\u0027\n",
        "FILENAME_HDI \u003d \u0027hdi.csv\u0027\n",
        "FILENAME_NEWS_HEADLINES \u003d \u0027news_headlines.csv\u0027\n",
        "FILENAME_POPULATION_DENSITY \u003d \u0027wrldbnk_pop_dnst.csv\u0027\n",
        "FILENAME_UNEMPLOYMENT \u003d \u0027wrldbnk_unemployment.csv\u0027\n",
        "FILENAME_URBAN_DENSITY \u003d \u0027wrldbnk_urban_pop.csv\u0027\n",
        "\n",
        "# Years\n",
        "DATE_START \u003d \u00272005\u0027\n",
        "DATE_END \u003d \u00272018\u0027\n",
        "\n",
        "DATE_RANGE \u003d [str(i) for i in range(2005, 2018)]\n",
        "DATE_NEWS_FROM \u003d [\u0027{}-02-02\u0027, \u0027{}-05-05\u0027, \u0027{}-07-07\u0027, \u0027{}-11-11\u0027]\n",
        "DATE_NEWS_TO \u003d [\u0027{}-03-03\u0027, \u0027{}-06-06\u0027,\u0027{}-08-08\u0027, \u0027{}-12-12\u0027]\n",
        "\n",
        "\n",
        "# country values\n",
        "SELECTED_COUNTRIES \u003d [\n",
        "    \u0027south africa\u0027,\n",
        "    \u0027kenya\u0027,\n",
        "    \u0027china\u0027,\n",
        "    \u0027taiwan\u0027,\n",
        "    \u0027japan\u0027,\n",
        "    \u0027south korea\u0027,\n",
        "    \u0027india\u0027,\n",
        "    \u0027pakistan\u0027,\n",
        "    \u0027indonesia\u0027,\n",
        "    \u0027philippines\u0027,\n",
        "    \u0027singapore\u0027,\n",
        "    \u0027thailand\u0027,\n",
        "    \u0027canada\u0027,\n",
        "    \u0027united kingdom\u0027,\n",
        "    \u0027ireland\u0027,\n",
        "    \u0027scotland\u0027,\n",
        "    \u0027australia\u0027,\n",
        "    \u0027new zealand\u0027,\n",
        "    \u0027united states\u0027,\n",
        "]\n",
        "\n",
        "\n",
        "COUNTRIES_DICT \u003d {\n",
        "    \u0027australia\u0027 : \u0027australia\u0027,\n",
        "    \u0027canada\u0027 : \u0027canada\u0027,\n",
        "    \u0027china\u0027 : \u0027asia/china\u0027,\n",
        "    \u0027india\u0027 : \u0027asia/india\u0027,\n",
        "    \u0027indonesia\u0027 : \u0027asia/southeast/indonesia\u0027,\n",
        "    \u0027ireland\u0027 : \u0027europe/ireland\u0027,\n",
        "    \u0027japan\u0027 : \u0027asia/japan\u0027,\n",
        "    \u0027kenya\u0027 : \u0027africa/kenya\u0027,\n",
        "    \u0027new zealand\u0027 : \u0027new_zealand\u0027,\n",
        "    \u0027pakistan\u0027 : \u0027asia/pakistan\u0027,\n",
        "    \u0027philippines\u0027 : \u0027asia/philippines\u0027,\n",
        "    \u0027scotland\u0027 : \u0027europe/scotland\u0027,\n",
        "    \u0027singapore\u0027 : \u0027asia/singapore\u0027,\n",
        "    \u0027south africa\u0027 : \u0027africa/south_africa\u0027,\n",
        "    \u0027south korea\u0027 : \u0027asia/south_korea\u0027,\n",
        "    \u0027taiwan\u0027 : \u0027asia/taiwan\u0027,\n",
        "    \u0027thailand\u0027 : \u0027asia/thailand\u0027,\n",
        "    \u0027united kingdom\u0027 : \u0027europe/uk\u0027,\n",
        "    \u0027united states\u0027 : \u0027us\u0027\n",
        "}\n",
        "\n",
        "\n",
        "# News URL\n",
        "URL \u003d \u0027https://newslookup.com/{}?\u0026ut\u003d{}\u0026l\u003d1\u0026utto\u003d{}\u0027\n",
        "FOLDS \u003d 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Headline Sentiment Data\n",
        "Headlines were collected for a set of countries by year. In order to use these headlines in further analysis, we want to calculate each headline\u0027s polarity and subjectivity and determine a mean for the year.\n",
        "\n",
        "Headlines are organized in a csv file with the following column headers:\n",
        "\n",
        "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
        "\n",
        "Together, the year and country columns are used as the index for the data.\n",
        "The analysis below creates three output dataframes, one for each headline\u0027s polarity, one for each headline\u0027s subjectivity, and one with average values of polarity and subjectivity for each year, per country. The headers of each of these are listed below.\n",
        "\n",
        "polarity_df and subjectivity_df:\n",
        "\n",
        "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
        "\n",
        "average_sentiment_df:\n",
        "\n",
        "    | year | country | polarity | subjectivity |\n",
        "\n",
        "Both of the dataframes are indexed by year and country.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Read in the parsed news headlines from the csv file\n",
        "file_path \u003d os.path.join(DIR_NEWS, FILENAME_NEWS_HEADLINES)\n",
        "headlines_df \u003d pd.read_csv(file_path, sep\u003d\u0027|\u0027, index_col\u003d(0, 1))\n",
        "\n",
        "average_sentiment_columns \u003d [COL_YEAR, COL_COUNTRY, COL_POLARITY, COL_SUBJECTIVITY]\n",
        "average_sentiment_df \u003d pd.DataFrame(columns\u003daverage_sentiment_columns)\n",
        "\n",
        "news_columns \u003d [COL_NEWS_TEMPLATE.format(i) for i in range(200)]\n",
        "news_columns.insert(0, COL_COUNTRY)\n",
        "news_columns.insert(0, COL_YEAR)\n",
        "polarity_df \u003d pd.DataFrame(columns\u003dnews_columns)\n",
        "subjectivity_df \u003d pd.DataFrame(columns\u003dnews_columns)\n",
        "\n",
        "# iterate through the headline rows\n",
        "for index, row in headlines_df.iterrows():\n",
        "    # lists to store the individual values for each headline\n",
        "    polarity_list \u003d list()\n",
        "    subjectivity_list \u003d list()\n",
        "\n",
        "    polarity_list.extend([index[0], index[1]])\n",
        "    subjectivity_list.extend([index[0], index[1]])\n",
        "\n",
        "    # values for the avgerage yearly polarity and subjectivity\n",
        "    yearly_average_polarity \u003d 0\n",
        "    yearly_average_subjectivity \u003d 0\n",
        "    yearly_average_count \u003d 0\n",
        "\n",
        "    # calculate polarity and subjectivity for each headline\n",
        "    for entry in row:\n",
        "        if type(entry) \u003d\u003d float:\n",
        "            polarity_list.append(entry)\n",
        "            subjectivity_list.append(entry)\n",
        "\n",
        "        else:\n",
        "            blob \u003d TextBlob(entry)\n",
        "\n",
        "            pol_val \u003d 0\n",
        "            sub_val \u003d 0\n",
        "            count \u003d 0\n",
        "\n",
        "            # average the values in case a headline is multiple sentences\n",
        "            for sentence in blob.sentences:\n",
        "                pol_val \u003d pol_val + sentence.sentiment.polarity\n",
        "                sub_val \u003d sub_val + sentence.sentiment.subjectivity\n",
        "                count \u003d count + 1\n",
        "\n",
        "            polarity_list.append(pol_val / count)\n",
        "            subjectivity_list.append(sub_val / count)\n",
        "\n",
        "            yearly_average_polarity \u003d yearly_average_polarity + pol_val / count\n",
        "            yearly_average_subjectivity \u003d yearly_average_subjectivity + sub_val / count\n",
        "            yearly_average_count \u003d yearly_average_count + 1\n",
        "\n",
        "    yearly_average_polarity \u003d yearly_average_polarity / yearly_average_count\n",
        "    yearly_average_subjectivity \u003d yearly_average_subjectivity / yearly_average_count\n",
        "\n",
        "    yearly_average_df \u003d pd.DataFrame([[index[0], index[1], yearly_average_polarity, yearly_average_subjectivity]], columns\u003daverage_sentiment_columns)\n",
        "\n",
        "    pol_row_df \u003d pd.DataFrame([polarity_list], columns\u003dnews_columns)\n",
        "    sub_row_df \u003d pd.DataFrame([subjectivity_list], columns\u003dnews_columns)\n",
        "\n",
        "    average_sentiment_df \u003d pd.concat([average_sentiment_df, yearly_average_df], sort\u003dFalse)\n",
        "    polarity_df \u003d pd.concat([polarity_df, pol_row_df], sort\u003dFalse)\n",
        "    subjectivity_df \u003d pd.concat([subjectivity_df, sub_row_df], sort\u003dFalse)\n",
        "\n",
        "# these are all of the polarities and subjectivities for each headline\n",
        "polarity_df \u003d polarity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
        "subjectivity_df \u003d subjectivity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
        "average_sentiment_df \u003d average_sentiment_df.set_index([COL_YEAR, COL_COUNTRY])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def load_data_frame(file_name, path, sep\u003dNone):\n",
        "    \"\"\"\n",
        "    Loads data from specified path and name, returns a dataframe\n",
        "    \"\"\"\n",
        "    file_path \u003d os.path.join(path, file_name)\n",
        "    if not sep:\n",
        "        return pd.read_csv(file_path)\n",
        "    return pd.read_csv(file_path, sep\u003dsep)\n",
        "\n",
        "\n",
        "def load_mental_health_data():\n",
        "    \"\"\"\n",
        "    Loads mental health data, and performs basic preprocessing operations:\n",
        "        - columns renamed appropriately for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "    \"\"\"\n",
        "    mental_df \u003d load_data_frame(\n",
        "        FILENAME_DISORDERS,\n",
        "        DIR_MENTAL_HEALTH\n",
        "    )\n",
        "\n",
        "    mental_df.rename(columns\u003d{\u0027Entity\u0027: COL_COUNTRY, \u0027Year\u0027: COL_YEAR}, inplace\u003dTrue)\n",
        "    mental_df.drop(labels\u003d\u0027Code\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "    mental_df[COL_COUNTRY] \u003d mental_df[COL_COUNTRY].str.lower()\n",
        "    mental_df \u003d mental_df[mental_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "    return mental_df\n",
        "\n",
        "\n",
        "def load_world_bank_data(filename, directory, value):\n",
        "    \"\"\"\n",
        "    Loads World Bank data, and performs basic preprocessing operations:\n",
        "        - columns renamed appropriately for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "        - non-numerical values are replaced with NaN or transformed appropriately\n",
        "    \"\"\"\n",
        "    df \u003d load_data_frame(\n",
        "        filename,\n",
        "        directory\n",
        "    )\n",
        "    df.rename(columns\u003d{\u0027Country Name\u0027: COL_COUNTRY}, inplace\u003dTrue)\n",
        "    df.drop([\u0027Indicator Name\u0027, \u0027Indicator Code\u0027, \u0027Country Code\u0027], axis\u003d1, inplace \u003d True)\n",
        "    df \u003d df.replace(\u0027..\u0027, np.NaN)\n",
        "    df.loc[:,1:] \u003d df.iloc[:, 1:].apply(pd.to_numeric)\n",
        "    df[COL_COUNTRY] \u003d df[COL_COUNTRY].str.lower()\n",
        "    df \u003d df.replace(\u0027korea, rep.\u0027, \u0027south korea\u0027)\n",
        "    df \u003d df.loc[df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "\n",
        "    columns \u003d df.columns\n",
        "    country_index \u003d columns.get_loc(COL_COUNTRY)\n",
        "    start_index \u003d columns.get_loc(DATE_START)\n",
        "    end_index \u003d columns.get_loc(DATE_END)\n",
        "    df \u003d df.iloc[:, np.r_[country_index, start_index:end_index]]\n",
        "\n",
        "    # Reshape the dataframe to structure `country|year|value`\n",
        "    df \u003d pd.melt(\n",
        "        df,\n",
        "        id_vars\u003dCOL_COUNTRY,\n",
        "        var_name\u003dCOL_YEAR,\n",
        "        value_name\u003dvalue\n",
        "    )\n",
        "    df[COL_YEAR] \u003d df[COL_YEAR].apply(pd.to_numeric)\n",
        "    df \u003d df.sort_values([COL_COUNTRY, COL_YEAR])\n",
        "    df.reset_index(inplace\u003dTrue, drop\u003dTrue)\n",
        "    df.dropna(axis\u003d0, inplace\u003dTrue)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_hdi_data():\n",
        "    \"\"\"\n",
        "    Loads human development index (HDI) data, and performs basic preprocessing operations:\n",
        "        - columns are renamed for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "    \"\"\"\n",
        "    hdi_df \u003d load_data_frame(\n",
        "        FILENAME_HDI,\n",
        "        DIR_HDI\n",
        "    )\n",
        "\n",
        "    hdi_df \u003d hdi_df.dropna(how\u003d\u0027all\u0027, axis\u003d1)\n",
        "    hdi_df.drop(\u0027HDI Rank (2017)\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "    hdi_df.rename(columns\u003d{\u0027Country\u0027: COL_COUNTRY, \u0027Year\u0027: COL_YEAR}, inplace\u003dTrue)\n",
        "    hdi_df[COL_COUNTRY] \u003d hdi_df[COL_COUNTRY].str.lower().str.strip()\n",
        "    hdi_df \u003d hdi_df[hdi_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "    hdi_df \u003d hdi_df.reset_index(drop\u003dTrue)\n",
        "    hdi_df.loc[:,1:] \u003d hdi_df.iloc[:,1:].apply(pd.to_numeric)\n",
        "\n",
        "    # Reshape the dataframe to structure `country|year|value`\n",
        "    hdi_df \u003d pd.melt(\n",
        "        hdi_df,\n",
        "        id_vars\u003d[COL_COUNTRY],\n",
        "        var_name\u003dCOL_YEAR,\n",
        "        value_name\u003dCOL_HDI\n",
        "    )\n",
        "\n",
        "    hdi_df[COL_YEAR] \u003d hdi_df[COL_YEAR].apply(pd.to_numeric)\n",
        "    hdi_df \u003d hdi_df.drop(hdi_df[hdi_df.year \u003c int(DATE_START)].index)\n",
        "    hdi_df \u003d hdi_df.drop(hdi_df[hdi_df.year \u003e int(DATE_END)].index)\n",
        "    return hdi_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Joining Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "population_df \u003d load_world_bank_data(FILENAME_POPULATION_DENSITY, DIR_POPULATION, COL_POPULATION_DENSITY)\n",
        "urban_df \u003d load_world_bank_data(FILENAME_URBAN_DENSITY, DIR_POPULATION, COL_URBAN_DENSITY)\n",
        "gdp_df \u003d load_world_bank_data(FILENAME_GDP, DIR_FINANCIAL, COL_GDP)\n",
        "unemployment_df \u003d load_world_bank_data(FILENAME_UNEMPLOYMENT, DIR_FINANCIAL, COL_UNEMPLOYMENT)\n",
        "hdi_df \u003d load_hdi_data()\n",
        "mental_df \u003d load_mental_health_data()\n",
        "\n",
        "\n",
        "\n",
        "# Joining individual datasets on population and urban population density, \n",
        "# GDP, unemployment, HDI, average news headlines sentiment, and mental health data\n",
        "data_frames_list \u003d [\n",
        "    population_df,\n",
        "    urban_df,\n",
        "    gdp_df,\n",
        "    unemployment_df,\n",
        "    hdi_df,\n",
        "    average_sentiment_df, \n",
        "    mental_df\n",
        "]\n",
        "\n",
        "joined_df \u003d reduce(lambda left, right: pd.merge(left, right, on\u003d[COL_COUNTRY, COL_YEAR], how\u003d\u0027inner\u0027), data_frames_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Pipeline Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "# Regressors and their tuning parameters\nnet \u003d MLPRegressor()\nnet_parameters \u003d {\n    \u0027net__learning_rate\u0027: [\u0027constant\u0027, \u0027invscaling\u0027, \u0027adaptive\u0027],\n    \u0027net__hidden_layer_sizes\u0027: [(50), (100), (50, 50), (100, 100), (150, 150), (100, 100, 100)],\n    \u0027net__max_iter\u0027: [2, 10, 100, 1000]\n}\n\nlinear \u003d LinearRegression()\nlinear_parameters \u003d {\n    \u0027linear__fit_intercept\u0027: [True, False]\n}\n\nforest \u003d RandomForestRegressor()\nforest_parameters \u003d {\n    \u0027forest__n_estimators\u0027: [10, 20, 50, 70, 100],\n    \u0027forest__max_depth\u0027: [2, 5, 10],\n    \u0027forest__min_samples_split\u0027: [2, 3, 4, 5],\n    \u0027forest__max_features\u0027: [\u0027auto\u0027, \u0027sqrt\u0027, \u0027log2\u0027]\n}\n\nstochastic \u003d SGDRegressor()\nstochastic_parameters \u003d {\n    \u0027stochastic__alpha\u0027: [0.0001, 0.001, 0.01, 0.1],\n    \u0027stochastic__penalty\u0027: [\u0027l2\u0027,\u0027l1\u0027],\n    \u0027stochastic__learning_rate\u0027: [\u0027invscaling\u0027, \u0027optimal\u0027, \u0027constant\u0027, \u0027adaptive\u0027]\n}\n\nknn \u003d KNeighborsRegressor()\nknn_parameters \u003d {\n    \u0027knn__n_neighbors\u0027: [5, 10, 20, 30]\n}\n\n# Scalers\nscaler \u003d MinMaxScaler()\n\n# Feature Selectors\nfeature_selector \u003d SelectKBest()\nselector_parameters \u003d {\n    \u0027selector__k\u0027: [\u0027all\u0027, 1, 2, 3, 4, 5, 6]\n}\n\n# Pipelines\nnet_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027net\u0027, net)])\nlinear_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027linear\u0027, linear)])\nforest_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027forest\u0027, forest)])\nstochastic_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027stochastic\u0027, stochastic)])\nknn_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027knn\u0027, knn)])\n\n\ndef get_grid_search(pipeline, parameters):\n    return GridSearchCV(pipeline, parameters, cv\u003dFOLDS, n_jobs\u003d-1, verbose\u003d5)\n    \n\ngrid_searcher_net \u003d get_grid_search(net_pipeline, dict(net_parameters, **selector_parameters))\n# grid_searcher_linear \u003d get_grid_search(linear_pipeline, dict(selector_parameters, **linear_parameters))\n# grid_searcher_forest \u003d get_grid_search(forest_pipeline, dict(selector_parameters, **forest_parameters))\n# grid_search_stochastic \u003d get_grid_search(stochastic_pipeline, dict(selector_parameters, **stochastic_parameters))\n# grid_searcher_knn \u003d get_grid_search(knn_pipeline, dict(selector_parameters, **knn_parameters))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index([\u0027country\u0027, \u0027year\u0027, \u0027population density\u0027, \u0027urban density (%)\u0027, \u0027GDP\u0027,\n       \u0027unemployment rate\u0027, \u0027HDI\u0027, \u0027polarity\u0027, \u0027subjectivity\u0027,\n       \u0027Schizophrenia (%)\u0027, \u0027Bipolar disorder (%)\u0027, \u0027Eating disorders (%)\u0027,\n       \u0027Anxiety disorders (%)\u0027, \u0027Drug use disorders (%)\u0027, \u0027Depression (%)\u0027,\n       \u0027Alcohol use disorders (%)\u0027],\n      dtype\u003d\u0027object\u0027)"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 35
        }
      ],
      "source": [
        "joined_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cb\u003eAnxiety Disorders\u003c/b\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 504 candidates, totalling 2520 fits\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  10 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 142 tasks      | elapsed:   13.3s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 295 tasks      | elapsed:   27.9s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 609 tasks      | elapsed:   59.5s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 967 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1326 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1748 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 2239 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 2513 out of 2520 | elapsed:  3.1min remaining:    0.5s\n[Parallel(n_jobs\u003d-1)]: Done 2520 out of 2520 | elapsed:  3.1min finished\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "GridSearchCV(cv\u003d5, error_score\u003d\u0027raise-deprecating\u0027,\n       estimator\u003dPipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d10, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027net\u0027, MLPRegressor(activation\u003d\u0027relu\u0027, alpha\u003d0.0001, batch_size\u003d\u0027auto\u0027, beta_1\u003d0.9,\n       beta_2\u003d0.999, early_stopping\u003dFalse, epsilon\u003d1e-08,\n       hid...\u003dTrue, solver\u003d\u0027adam\u0027, tol\u003d0.0001,\n       validation_fraction\u003d0.1, verbose\u003dFalse, warm_start\u003dFalse))]),\n       fit_params\u003dNone, iid\u003d\u0027warn\u0027, n_jobs\u003d-1,\n       param_grid\u003d{\u0027net__learning_rate\u0027: [\u0027constant\u0027, \u0027invscaling\u0027, \u0027adaptive\u0027], \u0027net__hidden_layer_sizes\u0027: [50, 100, (50, 50), (100, 100), (150, 150), (100, 100, 100)], \u0027net__max_iter\u0027: [2, 10, 100, 1000], \u0027selector__k\u0027: [\u0027all\u0027, 1, 2, 3, 4, 5, 6]},\n       pre_dispatch\u003d\u00272*n_jobs\u0027, refit\u003dTrue, return_train_score\u003d\u0027warn\u0027,\n       scoring\u003dNone, verbose\u003d5)"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 36
        }
      ],
      "source": "# Removing indexing columns (country, year) and rates that will not be predicted\nTARGET \u003d \u0027Anxiety disorders (%)\u0027\n\ny \u003d joined_df.copy().pop(TARGET)\nx \u003d joined_df.copy()[FEATURES]\n\nx_train, x_test, y_train, y_test \u003d train_test_split(x, y, random_state\u003d0)\ngrid_searcher_net.fit(x_train, y_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Best estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d6, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027net\u0027, MLPRegressor(activation\u003d\u0027relu\u0027, alpha\u003d0.0001, batch_size\u003d\u0027auto\u0027, beta_1\u003d0.9,\n       beta_2\u003d0.999, early_stopping\u003dFalse, epsilon\u003d1e-08,\n       hidd...\u003dTrue, solver\u003d\u0027adam\u0027, tol\u003d0.0001,\n       validation_fraction\u003d0.1, verbose\u003dFalse, warm_start\u003dFalse))])\nBest score: 0.8154689346302422\nBest parameters: {\u0027net__hidden_layer_sizes\u0027: (50, 50), \u0027net__learning_rate\u0027: \u0027constant\u0027, \u0027net__max_iter\u0027: 1000, \u0027selector__k\u0027: 6}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(\u0027Best estimator:\u0027, grid_searcher_net.best_estimator_)\nprint(\u0027Best score:\u0027, grid_searcher_net.best_score_)\nprint(\u0027Best parameters:\u0027, grid_searcher_net.best_params_)\n\nnet \u003d grid_searcher_net.classifier.best_estimator_.named_steps.net\n\nif not self.features:\n    num_features_used \u003d self.classifier.best_params_[\u0027selector__k\u0027]\n    scores \u003d self.classifier.best_estimator_.named_steps.selector.scores_\n    highest_scores_indices \u003d numpy.argpartition(scores, -num_features_used)[-num_features_used:]\n\n    features_used \u003d x_train.columns[highest_scores_indices].values"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}