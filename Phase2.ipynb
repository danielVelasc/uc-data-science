{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Multi-Factor Prediction of Mental Illness Incidence Rates\n",
    "**Quinn Bischoff, Eric Matteucci, Rajat Singh, Daniel Velasco**\n",
    "\n",
    "# Phase II\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Behavioural and emotional well-being is integral to the development of societies around the world. However, the rates of incidence of mental health disorders are on the rise in some places around the globe, while others are declining. Our goal is to predict these incidence rates using linear regression and deep learning methods on a rich data set.\n",
    "\n",
    "Using a combination of data sets that include news headlines, financial indicators, and population distributions and indices, to generate a prediction of incidence of disorders such as depression or anxiety, and deaths by mental health. To this end, news headlines that originate from a given country will be preprocessed using natural language processing (NLP)â€”specifically, sentiment analysis. The score, in conjunction with the aforementioned datasets, will be used to generate a linear regression model and a neural network. The ultimate goal of this project is to determine whether news headline sentiments from a country are accurate at predicting the mental health disorder rate of the country.\n",
    "\n",
    "## Phase II Goal\n",
    "The aim of this phase is to ...\n",
    "\n",
    "[X] methods will be employed:\n",
    "1. First\n",
    "2. ...\n",
    "X. Last\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "COL_COUNTRY = 'country'\n",
    "COL_GDP = 'GDP'\n",
    "COL_HDI = 'HDI'\n",
    "COL_POLARITY = 'polarity'\n",
    "COL_POPULATION_DENSITY = 'population density'\n",
    "COL_SUBJECTIVITY = 'subjectivity'\n",
    "COL_UNEMPLOYMENT = 'unemployment rate'\n",
    "COL_URBAN_DENSITY = 'urban density (%)'\n",
    "COL_YEAR = 'year'\n",
    "COL_NEWS_TEMPLATE = 'news{}'\n",
    "\n",
    "TARGETS = [\n",
    "    'Bipolar disorder (%)',\n",
    "    'Eating disorders (%)',\n",
    "    'Anxiety disorders (%)',\n",
    "    'Drug use disorders (%)',\n",
    "    'Depression (%)',\n",
    "    'Alcohol use disorders (%)',\n",
    "]\n",
    "\n",
    "FEATURES = [\n",
    "    COL_GDP,\n",
    "    COL_HDI,\n",
    "    COL_POLARITY,\n",
    "    COL_POPULATION_DENSITY,\n",
    "    COL_SUBJECTIVITY,\n",
    "    COL_UNEMPLOYMENT,\n",
    "    COL_URBAN_DENSITY,\n",
    "]\n",
    "\n",
    "# directories and filenames\n",
    "DIR_DATA = 'data'\n",
    "DIR_FINANCIAL = os.path.join(DIR_DATA, 'financial')\n",
    "DIR_HDI = os.path.join(DIR_DATA, 'human_development_index')\n",
    "DIR_MENTAL_HEALTH = os.path.join(DIR_DATA, 'mental_health')\n",
    "DIR_NEWS = os.path.join(DIR_DATA, 'news')\n",
    "DIR_POPULATION = os.path.join(DIR_DATA, 'population')\n",
    "\n",
    "FILENAME_DISORDERS = 'prevalence-by-mental-and-substance-use-disorder.csv'\n",
    "FILENAME_GDP = 'wrldbnk_gdp.csv'\n",
    "FILENAME_HDI = 'hdi.csv'\n",
    "FILENAME_NEWS_HEADLINES = 'news_headlines.csv'\n",
    "FILENAME_POPULATION_DENSITY = 'wrldbnk_pop_dnst.csv'\n",
    "FILENAME_UNEMPLOYMENT = 'wrldbnk_unemployment.csv'\n",
    "FILENAME_URBAN_DENSITY = 'wrldbnk_urban_pop.csv'\n",
    "\n",
    "# Years\n",
    "DATE_START = '2005'\n",
    "DATE_END = '2018'\n",
    "\n",
    "DATE_RANGE = [str(i) for i in range(2005, 2018)]\n",
    "DATE_NEWS_FROM = ['{}-02-02', '{}-05-05', '{}-07-07', '{}-11-11']\n",
    "DATE_NEWS_TO = ['{}-03-03', '{}-06-06','{}-08-08', '{}-12-12']\n",
    "\n",
    "\n",
    "# country values\n",
    "SELECTED_COUNTRIES = [\n",
    "    'south africa',\n",
    "    'kenya',\n",
    "    'china',\n",
    "    'taiwan',\n",
    "    'japan',\n",
    "    'south korea',\n",
    "    'india',\n",
    "    'pakistan',\n",
    "    'indonesia',\n",
    "    'philippines',\n",
    "    'singapore',\n",
    "    'thailand',\n",
    "    'canada',\n",
    "    'united kingdom',\n",
    "    'ireland',\n",
    "    'scotland',\n",
    "    'australia',\n",
    "    'new zealand',\n",
    "    'united states',\n",
    "]\n",
    "\n",
    "\n",
    "COUNTRIES_DICT = {\n",
    "    'australia' : 'australia',\n",
    "    'canada' : 'canada',\n",
    "    'china' : 'asia/china',\n",
    "    'india' : 'asia/india',\n",
    "    'indonesia' : 'asia/southeast/indonesia',\n",
    "    'ireland' : 'europe/ireland',\n",
    "    'japan' : 'asia/japan',\n",
    "    'kenya' : 'africa/kenya',\n",
    "    'new zealand' : 'new_zealand',\n",
    "    'pakistan' : 'asia/pakistan',\n",
    "    'philippines' : 'asia/philippines',\n",
    "    'scotland' : 'europe/scotland',\n",
    "    'singapore' : 'asia/singapore',\n",
    "    'south africa' : 'africa/south_africa',\n",
    "    'south korea' : 'asia/south_korea',\n",
    "    'taiwan' : 'asia/taiwan',\n",
    "    'thailand' : 'asia/thailand',\n",
    "    'united kingdom' : 'europe/uk',\n",
    "    'united states' : 'us'\n",
    "}\n",
    "\n",
    "\n",
    "# News URL\n",
    "URL = 'https://newslookup.com/{}?&ut={}&l=1&utto={}'\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Headline Sentiment Data\n",
    "Headlines were collected for a set of countries by year. In order to use these headlines in further analysis, we want to calculate each headline's polarity and subjectivity and determine a mean for the year.\n",
    "\n",
    "Headlines are organized in a csv file with the following column headers:\n",
    "\n",
    "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
    "\n",
    "Together, the year and country columns are used as the index for the data.\n",
    "The analysis below creates three output dataframes, one for each headline's polarity, one for each headline's subjectivity, and one with average values of polarity and subjectivity for each year, per country. The headers of each of these are listed below.\n",
    "\n",
    "polarity_df and subjectivity_df:\n",
    "\n",
    "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
    "\n",
    "average_sentiment_df:\n",
    "\n",
    "    | year | country | polarity | subjectivity |\n",
    "\n",
    "Both of the dataframes are indexed by year and country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in the parsed news headlines from the csv file\n",
    "file_path = os.path.join(DIR_NEWS, FILENAME_NEWS_HEADLINES)\n",
    "headlines_df = pd.read_csv(file_path, sep='|', index_col=(0, 1))\n",
    "\n",
    "average_sentiment_columns = [COL_YEAR, COL_COUNTRY, COL_POLARITY, COL_SUBJECTIVITY]\n",
    "average_sentiment_df = pd.DataFrame(columns=average_sentiment_columns)\n",
    "\n",
    "news_columns = [COL_NEWS_TEMPLATE.format(i) for i in range(200)]\n",
    "news_columns.insert(0, COL_COUNTRY)\n",
    "news_columns.insert(0, COL_YEAR)\n",
    "polarity_df = pd.DataFrame(columns=news_columns)\n",
    "subjectivity_df = pd.DataFrame(columns=news_columns)\n",
    "\n",
    "# iterate through the headline rows\n",
    "for index, row in headlines_df.iterrows():\n",
    "    # lists to store the individual values for each headline\n",
    "    polarity_list = list()\n",
    "    subjectivity_list = list()\n",
    "\n",
    "    polarity_list.extend([index[0], index[1]])\n",
    "    subjectivity_list.extend([index[0], index[1]])\n",
    "\n",
    "    # values for the avgerage yearly polarity and subjectivity\n",
    "    yearly_average_polarity = 0\n",
    "    yearly_average_subjectivity = 0\n",
    "    yearly_average_count = 0\n",
    "\n",
    "    # calculate polarity and subjectivity for each headline\n",
    "    for entry in row:\n",
    "        if type(entry) == float:\n",
    "            polarity_list.append(entry)\n",
    "            subjectivity_list.append(entry)\n",
    "\n",
    "        else:\n",
    "            blob = TextBlob(entry)\n",
    "\n",
    "            pol_val = 0\n",
    "            sub_val = 0\n",
    "            count = 0\n",
    "\n",
    "            # average the values in case a headline is multiple sentences\n",
    "            for sentence in blob.sentences:\n",
    "                pol_val = pol_val + sentence.sentiment.polarity\n",
    "                sub_val = sub_val + sentence.sentiment.subjectivity\n",
    "                count = count + 1\n",
    "\n",
    "            polarity_list.append(pol_val / count)\n",
    "            subjectivity_list.append(sub_val / count)\n",
    "\n",
    "            yearly_average_polarity = yearly_average_polarity + pol_val / count\n",
    "            yearly_average_subjectivity = yearly_average_subjectivity + sub_val / count\n",
    "            yearly_average_count = yearly_average_count + 1\n",
    "\n",
    "    yearly_average_polarity = yearly_average_polarity / yearly_average_count\n",
    "    yearly_average_subjectivity = yearly_average_subjectivity / yearly_average_count\n",
    "\n",
    "    yearly_average_df = pd.DataFrame([[index[0], index[1], yearly_average_polarity, yearly_average_subjectivity]], columns=average_sentiment_columns)\n",
    "\n",
    "    pol_row_df = pd.DataFrame([polarity_list], columns=news_columns)\n",
    "    sub_row_df = pd.DataFrame([subjectivity_list], columns=news_columns)\n",
    "\n",
    "    average_sentiment_df = pd.concat([average_sentiment_df, yearly_average_df], sort=False)\n",
    "    polarity_df = pd.concat([polarity_df, pol_row_df], sort=False)\n",
    "    subjectivity_df = pd.concat([subjectivity_df, sub_row_df], sort=False)\n",
    "\n",
    "# these are all of the polarities and subjectivity values for each headline\n",
    "polarity_df = polarity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
    "subjectivity_df = subjectivity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
    "average_sentiment_df = average_sentiment_df.set_index([COL_YEAR, COL_COUNTRY])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data_frame(file_name, path, sep=None):\n",
    "    \"\"\"\n",
    "    Loads data from specified path and name, returns a dataframe\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    if not sep:\n",
    "        return pd.read_csv(file_path)\n",
    "    return pd.read_csv(file_path, sep=sep)\n",
    "\n",
    "\n",
    "def load_mental_health_data():\n",
    "    \"\"\"\n",
    "    Loads mental health data, and performs basic preprocessing operations:\n",
    "        - columns renamed appropriately for compatibility\n",
    "        - selected countries are filtered\n",
    "        - unnecessary columns are dropped\n",
    "    \"\"\"\n",
    "    mental_df = load_data_frame(\n",
    "        FILENAME_DISORDERS,\n",
    "        DIR_MENTAL_HEALTH\n",
    "    )\n",
    "\n",
    "    mental_df.rename(columns={'Entity': COL_COUNTRY, 'Year': COL_YEAR}, inplace=True)\n",
    "    mental_df.drop(labels='Code', axis=1, inplace=True)\n",
    "    mental_df[COL_COUNTRY] = mental_df[COL_COUNTRY].str.lower()\n",
    "    mental_df = mental_df[mental_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
    "    return mental_df\n",
    "\n",
    "\n",
    "def load_world_bank_data(filename, directory, value):\n",
    "    \"\"\"\n",
    "    Loads World Bank data, and performs basic preprocessing operations:\n",
    "        - columns renamed appropriately for compatibility\n",
    "        - selected countries are filtered\n",
    "        - unnecessary columns are dropped\n",
    "        - non-numerical values are replaced with NaN or transformed appropriately\n",
    "    \"\"\"\n",
    "    df = load_data_frame(\n",
    "        filename,\n",
    "        directory\n",
    "    )\n",
    "    df.rename(columns={'Country Name': COL_COUNTRY}, inplace=True)\n",
    "    df.drop(['Indicator Name', 'Indicator Code', 'Country Code'], axis=1, inplace = True)\n",
    "    df = df.replace('..', np.NaN)\n",
    "    df.loc[:,1:] = df.iloc[:, 1:].apply(pd.to_numeric)\n",
    "    df[COL_COUNTRY] = df[COL_COUNTRY].str.lower()\n",
    "    df = df.replace('korea, rep.', 'south korea')\n",
    "    df = df.loc[df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
    "\n",
    "    columns = df.columns\n",
    "    country_index = columns.get_loc(COL_COUNTRY)\n",
    "    start_index = columns.get_loc(DATE_START)\n",
    "    end_index = columns.get_loc(DATE_END)\n",
    "    df = df.iloc[:, np.r_[country_index, start_index:end_index]]\n",
    "\n",
    "    # Reshape the dataframe to structure `country|year|value`\n",
    "    df = pd.melt(\n",
    "        df,\n",
    "        id_vars=COL_COUNTRY,\n",
    "        var_name=COL_YEAR,\n",
    "        value_name=value\n",
    "    )\n",
    "    df[COL_YEAR] = df[COL_YEAR].apply(pd.to_numeric)\n",
    "    df = df.sort_values([COL_COUNTRY, COL_YEAR])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_hdi_data():\n",
    "    \"\"\"\n",
    "    Loads human development index (HDI) data, and performs basic preprocessing operations:\n",
    "        - columns are renamed for compatibility\n",
    "        - selected countries are filtered\n",
    "        - unnecessary columns are dropped\n",
    "    \"\"\"\n",
    "    hdi_df = load_data_frame(\n",
    "        FILENAME_HDI,\n",
    "        DIR_HDI\n",
    "    )\n",
    "\n",
    "    hdi_df = hdi_df.dropna(how='all', axis=1)\n",
    "    hdi_df.drop('HDI Rank (2017)', axis=1, inplace=True)\n",
    "    hdi_df.rename(columns={'Country': COL_COUNTRY, 'Year': COL_YEAR}, inplace=True)\n",
    "    hdi_df[COL_COUNTRY] = hdi_df[COL_COUNTRY].str.lower().str.strip()\n",
    "    hdi_df = hdi_df[hdi_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
    "    hdi_df = hdi_df.reset_index(drop=True)\n",
    "    hdi_df.loc[:,1:] = hdi_df.iloc[:,1:].apply(pd.to_numeric)\n",
    "\n",
    "    # Reshape the dataframe to structure `country|year|value`\n",
    "    hdi_df = pd.melt(\n",
    "        hdi_df,\n",
    "        id_vars=[COL_COUNTRY],\n",
    "        var_name=COL_YEAR,\n",
    "        value_name=COL_HDI\n",
    "    )\n",
    "\n",
    "    hdi_df[COL_YEAR] = hdi_df[COL_YEAR].apply(pd.to_numeric)\n",
    "    hdi_df = hdi_df.drop(hdi_df[hdi_df.year < int(DATE_START)].index)\n",
    "    hdi_df = hdi_df.drop(hdi_df[hdi_df.year > int(DATE_END)].index)\n",
    "    return hdi_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "population_df = load_world_bank_data(FILENAME_POPULATION_DENSITY, DIR_POPULATION, COL_POPULATION_DENSITY)\n",
    "urban_df = load_world_bank_data(FILENAME_URBAN_DENSITY, DIR_POPULATION, COL_URBAN_DENSITY)\n",
    "gdp_df = load_world_bank_data(FILENAME_GDP, DIR_FINANCIAL, COL_GDP)\n",
    "unemployment_df = load_world_bank_data(FILENAME_UNEMPLOYMENT, DIR_FINANCIAL, COL_UNEMPLOYMENT)\n",
    "hdi_df = load_hdi_data()\n",
    "mental_df = load_mental_health_data()\n",
    "\n",
    "\n",
    "\n",
    "# Joining individual datasets on population and urban population density, \n",
    "# GDP, unemployment, HDI, average news headlines sentiment, and mental health data\n",
    "data_frames_list = [\n",
    "    population_df,\n",
    "    urban_df,\n",
    "    gdp_df,\n",
    "    unemployment_df,\n",
    "    hdi_df,\n",
    "    average_sentiment_df, \n",
    "    mental_df\n",
    "]\n",
    "\n",
    "joined_df = reduce(lambda left, right: pd.merge(left, right, on=[COL_COUNTRY, COL_YEAR], how='inner'), data_frames_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Regressors and their tuning parameters\n",
    "net = MLPRegressor()\n",
    "net_parameters = {\n",
    "    'net__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'net__hidden_layer_sizes': [(50), (100), (50, 50), (100, 100), (150, 150), (100, 100, 100)],\n",
    "    'net__max_iter': [10, 100, 1000]\n",
    "}\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear_parameters = {\n",
    "    'linear__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_parameters = {\n",
    "    'forest__n_estimators': [10, 20, 50],\n",
    "    'forest__max_depth': [2, 5],\n",
    "    'forest__min_samples_split': [3, 4, 5],\n",
    "    'forest__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "svr = SVR(kernel='rbf')\n",
    "svr_parameters = {\n",
    "    'svr__C': [0.001, 0.01,  1, 10, 100],\n",
    "    'svr__epsilon': [ 0.01, .1, 0.5, 1, 2, 4],\n",
    "}\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "knn_parameters = {\n",
    "    'knn__n_neighbors': [5, 10, 20, 30]\n",
    "}\n",
    "\n",
    "def generate_pipelined_grid_search(name_estimator, estimator_params, scale=True, pca=False):\n",
    "    \"\"\"\n",
    "    Generates a GridSearchCV object by assembling a Pipeline where input estimator is at the end. \n",
    "    Default behaviour is to include a 'SelectKBest' step between scaling and regression.\n",
    "    \n",
    "    :param name_estimator: a tuple of name and estimator to be used at the end of the pipeline\n",
    "    :param estimator_params: dictionary of parameters to tune in the estimator\n",
    "    :param scale: Whether to include a scaling component at the beginning of the pipeline\n",
    "    :param pca: If true, 'selector' step is associated to a PCA object\n",
    "    \n",
    "    :return: A pipeline containing input estimator and a dictionary of parameters to be tuned\n",
    "             for feature selection.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    if scale or pca:\n",
    "        steps.append(('scaler', MinMaxScaler()))\n",
    "    if pca:\n",
    "        steps.append(('selector', PCA(random_state=0)))\n",
    "        params = dict(estimator_params, selector__n_components=range(1, len(FEATURES)+1))\n",
    "\n",
    "    if not pca: \n",
    "        steps.append(('selector', SelectKBest()))\n",
    "        params = dict(estimator_params, selector__k=range(1, len(FEATURES)+1))\n",
    "\n",
    "    steps.append(name_estimator)\n",
    "\n",
    "    return GridSearchCV(\n",
    "        Pipeline(steps=steps),\n",
    "        params,\n",
    "        cv=FOLDS,\n",
    "        n_jobs=-1,\n",
    "        verbose=5\n",
    "    )\n",
    "\n",
    "def print_stats(grid_searcher, x_test, y_test, pca=False):\n",
    "    print('Best estimator\\n', grid_searcher.best_estimator_, '\\n')\n",
    "    print('Best parameters:', grid_searcher.best_params_, '\\n')\n",
    "    print('Train score:', grid_searcher.best_score_)\n",
    "    print('Test score:', grid_searcher.best_estimator_.score(x_test, y_test))\n",
    "    \n",
    "    if not pca:\n",
    "        num_features_used = int(grid_searcher.best_params_['selector__k'])\n",
    "        scores = grid_searcher.best_estimator_.named_steps.selector.scores_\n",
    "        highest_scores_indices = np.argpartition(scores, -num_features_used)[-num_features_used:]\n",
    "    \n",
    "        features_used = x_test.columns[highest_scores_indices].values\n",
    "        print('Features used:', features_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'year', 'population density', 'urban density (%)', 'GDP',\n",
       "       'unemployment rate', 'HDI', 'polarity', 'subjectivity',\n",
       "       'Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)',\n",
       "       'Anxiety disorders (%)', 'Drug use disorders (%)', 'Depression (%)',\n",
       "       'Alcohol use disorders (%)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Training Data\n",
      " Index(['GDP', 'HDI', 'polarity', 'population density', 'subjectivity',\n",
      "       'unemployment rate', 'urban density (%)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Removing indexing columns (country, year) and rates that will not be predicted\n",
    "TARGET = 'Anxiety disorders (%)'\n",
    "\n",
    "y = joined_df.copy().pop(TARGET)\n",
    "x = joined_df.copy()[FEATURES]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "print('Columns in Training Data\\n', x.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<b>SelectKBest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 318 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 522 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 843 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1167 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1635 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1883 out of 1890 | elapsed:  2.1min remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1890 out of 1890 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****\tMULTI-LAYER PERCEPTRON\t****\n",
      "Best estimator\n",
      " Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=4, score_func=<function f_classif at 0x11c916ea0>)), ('net', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidd...=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))]) \n",
      "\n",
      "Best parameters: {'net__hidden_layer_sizes': (50, 50), 'net__learning_rate': 'adaptive', 'net__max_iter': 1000, 'selector__k': 4} \n",
      "\n",
      "Train score: 0.7506462318245589\n",
      "Test score: 0.8808278467557366\n",
      "Features used: ['GDP' 'HDI' 'polarity' 'urban density (%)']\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  70 | elapsed:    0.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****\tLINEAR REGRESSOR\t****\n",
      "Best estimator\n",
      " Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=7, score_func=<function f_classif at 0x11c916ea0>)), ('linear', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False))]) \n",
      "\n",
      "Best parameters: {'linear__fit_intercept': True, 'selector__k': 7} \n",
      "\n",
      "Train score: 0.7039916938507242\n",
      "Test score: 0.8623185517949138\n",
      "Features used: ['GDP' 'HDI' 'polarity' 'population density' 'subjectivity'\n",
      " 'unemployment rate' 'urban density (%)']\n",
      "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 746 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1394 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1890 out of 1890 | elapsed:   34.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****\tRANDOM FOREST REGRESSOR\t****\n",
      "Best estimator\n",
      " Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=7, score_func=<function f_classif at 0x11c916ea0>)), ('forest', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurit...ators=50, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))]) \n",
      "\n",
      "Best parameters: {'forest__max_depth': 5, 'forest__max_features': 'auto', 'forest__min_samples_split': 3, 'forest__n_estimators': 50, 'selector__k': 7} \n",
      "\n",
      "Train score: 0.951989462667305\n",
      "Test score: 0.9802873153500465\n",
      "Features used: ['GDP' 'HDI' 'polarity' 'population density' 'subjectivity'\n",
      " 'unemployment rate' 'urban density (%)']\n",
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****\tSUPPORT VECTOR MACHINE\t****\n",
      "Best estimator\n",
      " Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=7, score_func=<function f_classif at 0x11c916ea0>)), ('svr', SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False))]) \n",
      "\n",
      "Best parameters: {'selector__k': 7, 'svr__C': 10, 'svr__epsilon': 0.1} \n",
      "\n",
      "Train score: 0.7357758163059576\n",
      "Test score: 0.8857402238638656\n",
      "Features used: ['GDP' 'HDI' 'polarity' 'population density' 'subjectivity'\n",
      " 'unemployment rate' 'urban density (%)']\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t****\tK-NEAREST NEIGHBORS\t****\n",
      "Best estimator\n",
      " Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', SelectKBest(k=7, score_func=<function f_classif at 0x11c916ea0>)), ('knn', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "          weights='uniform'))]) \n",
      "\n",
      "Best parameters: {'knn__n_neighbors': 5, 'selector__k': 7} \n",
      "\n",
      "Train score: 0.6760881468825559\n",
      "Test score: 0.897100082257263\n",
      "Features used: ['GDP' 'HDI' 'polarity' 'population density' 'subjectivity'\n",
      " 'unemployment rate' 'urban density (%)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "grid_searcher_net = generate_pipelined_grid_search(('net', net), net_parameters)\n",
    "grid_searcher_linear = generate_pipelined_grid_search(('linear', linear), linear_parameters)\n",
    "grid_searcher_forest = generate_pipelined_grid_search(('forest', forest), forest_parameters)\n",
    "grid_searcher_svr = generate_pipelined_grid_search(('svr', svr), svr_parameters)\n",
    "grid_searcher_knn = generate_pipelined_grid_search(('knn', knn), knn_parameters)\n",
    "\n",
    "grid_searcher_net.fit(x_train, y_train)\n",
    "print('\\t****\\tMULTI-LAYER PERCEPTRON\\t****')\n",
    "print_stats(grid_searcher_net, x_test, y_test)\n",
    "\n",
    "grid_searcher_linear.fit(x_train, y_train)\n",
    "print('\\t****\\tLINEAR REGRESSOR\\t****')\n",
    "print_stats(grid_searcher_linear, x_test, y_test)\n",
    "\n",
    "grid_searcher_forest.fit(x_train, y_train)\n",
    "print('\\t****\\tRANDOM FOREST REGRESSOR\\t****')\n",
    "print_stats(grid_searcher_forest, x_test, y_test)\n",
    "\n",
    "grid_searcher_svr.fit(x_train, y_train)\n",
    "print('\\t****\\tSUPPORT VECTOR MACHINE\\t****')\n",
    "print_stats(grid_searcher_svr, x_test, y_test)\n",
    "\n",
    "grid_searcher_knn.fit(x_train, y_train)\n",
    "print('\\t****\\tK-NEAREST NEIGHBORS\\t****')\n",
    "print_stats(grid_searcher_knn, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "<b>PCA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done 751 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1048 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1303 tasks      | elapsed:  1.6min\n"
     ]
    }
   ],
   "source": [
    "grid_searcher_net_pca = generate_pipelined_grid_search(('net', net), net_parameters, pca=True)\n",
    "grid_searcher_linear_pca = generate_pipelined_grid_search(('linear', linear), linear_parameters, pca=True)\n",
    "grid_searcher_forest_pca = generate_pipelined_grid_search(('forest', forest), forest_parameters, pca=True)\n",
    "grid_searcher_svr_pca = generate_pipelined_grid_search(('svr', svr), svr_parameters, pca=True)\n",
    "grid_searcher_knn_pca = generate_pipelined_grid_search(('knn', knn), knn_parameters, pca=True)\n",
    "\n",
    "grid_searcher_net_pca.fit(x_train, y_train)\n",
    "print('\\t****\\tMULTI-LAYER PERCEPTRON\\t****')\n",
    "print_stats(grid_searcher_net_pca, x_test, y_test, pca=True)\n",
    "\n",
    "grid_searcher_linear_pca.fit(x_train, y_train)\n",
    "print('\\t****\\tLINEAR REGRESSOR\\t****')\n",
    "print_stats(grid_searcher_linear_pca, x_test, y_test, pca=True)\n",
    "\n",
    "grid_searcher_forest_pca.fit(x_train, y_train)\n",
    "print('\\t****\\tRANDOM FOREST REGRESSOR\\t****')\n",
    "print_stats(grid_searcher_forest_pca, x_test, y_test, pca=True)\n",
    "\n",
    "grid_searcher_svr_pca.fit(x_train, y_train)\n",
    "print('\\t****\\tSUPPORT VECTOR MACHINE\\t****')\n",
    "print_stats(grid_searcher_svr_pca, x_test, y_test, pca=True)\n",
    "\n",
    "grid_searcher_knn_pca.fit(x_train, y_train)\n",
    "print('\\t****\\tK-NEAREST NEIGHBORS\\t****')\n",
    "print_stats(grid_searcher_knn_pca, x_test, y_test, pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
