{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Multi-Factor Prediction of Mental Illness Incidence Rates\n",
        "**Quinn Bischoff, Eric Matteucci, Rajat Singh, Daniel Velasco**\n",
        "\n",
        "# Phase II\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Behavioural and emotional well-being is integral to the development of societies around the world. However, the rates of incidence of mental health disorders are on the rise in some places around the globe, while others are declining. Our goal is to predict these incidence rates using linear regression and deep learning methods on a rich data set.\n",
        "\n",
        "Using a combination of data sets that include news headlines, financial indicators, and population distributions and indices, to generate a prediction of incidence of disorders such as depression or anxiety, and deaths by mental health. To this end, news headlines that originate from a given country will be preprocessed using natural language processing (NLP)â€”specifically, sentiment analysis. The score, in conjunction with the aforementioned datasets, will be used to generate a linear regression model and a neural network. The ultimate goal of this project is to determine whether news headline sentiments from a country are accurate at predicting the mental health disorder rate of the country.\n",
        "\n",
        "## Phase II Goal\n",
        "The aim of this phase is to ...\n",
        "\n",
        "[X] methods will be employed:\n",
        "1. First\n",
        "2. ...\n",
        "X. Last\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from functools import reduce\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ridge\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom textblob import TextBlob\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as grid\nimport nltk\nimport numpy as np\nimport os\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "# column names\nCOL_COUNTRY \u003d \u0027country\u0027\nCOL_GDP \u003d \u0027GDP\u0027\nCOL_HDI \u003d \u0027HDI\u0027\nCOL_POLARITY \u003d \u0027polarity\u0027\nCOL_POPULATION_DENSITY \u003d \u0027population density\u0027\nCOL_SUBJECTIVITY \u003d \u0027subjectivity\u0027\nCOL_UNEMPLOYMENT \u003d \u0027unemployment rate\u0027\nCOL_URBAN_DENSITY \u003d \u0027urban density (%)\u0027\nCOL_YEAR \u003d \u0027year\u0027\nCOL_NEWS_TEMPLATE \u003d \u0027news{}\u0027\n\nTARGETS \u003d [\n    \u0027Bipolar disorder (%)\u0027,\n    \u0027Eating disorders (%)\u0027,\n    \u0027Anxiety disorders (%)\u0027,\n    \u0027Drug use disorders (%)\u0027,\n    \u0027Depression (%)\u0027,\n    \u0027Alcohol use disorders (%)\u0027,\n]\n\nFEATURES \u003d [\n    COL_GDP,\n    COL_HDI,\n    COL_POLARITY,\n    COL_POPULATION_DENSITY,\n    COL_SUBJECTIVITY,\n    COL_UNEMPLOYMENT,\n    COL_URBAN_DENSITY,\n]\n\n# directories and filenames\nDIR_DATA \u003d \u0027data\u0027\nDIR_FINANCIAL \u003d os.path.join(DIR_DATA, \u0027financial\u0027)\nDIR_HDI \u003d os.path.join(DIR_DATA, \u0027human_development_index\u0027)\nDIR_MENTAL_HEALTH \u003d os.path.join(DIR_DATA, \u0027mental_health\u0027)\nDIR_NEWS \u003d os.path.join(DIR_DATA, \u0027news\u0027)\nDIR_POPULATION \u003d os.path.join(DIR_DATA, \u0027population\u0027)\n\nFILENAME_DISORDERS \u003d \u0027prevalence-by-mental-and-substance-use-disorder.csv\u0027\nFILENAME_GDP \u003d \u0027wrldbnk_gdp.csv\u0027\nFILENAME_HDI \u003d \u0027hdi.csv\u0027\nFILENAME_NEWS_HEADLINES \u003d \u0027news_headlines.csv\u0027\nFILENAME_POPULATION_DENSITY \u003d \u0027wrldbnk_pop_dnst.csv\u0027\nFILENAME_UNEMPLOYMENT \u003d \u0027wrldbnk_unemployment.csv\u0027\nFILENAME_URBAN_DENSITY \u003d \u0027wrldbnk_urban_pop.csv\u0027\n\n# Years\nDATE_START \u003d \u00272005\u0027\nDATE_END \u003d \u00272018\u0027\n\nDATE_RANGE \u003d [str(i) for i in range(2005, 2018)]\nDATE_NEWS_FROM \u003d [\u0027{}-02-02\u0027, \u0027{}-05-05\u0027, \u0027{}-07-07\u0027, \u0027{}-11-11\u0027]\nDATE_NEWS_TO \u003d [\u0027{}-03-03\u0027, \u0027{}-06-06\u0027,\u0027{}-08-08\u0027, \u0027{}-12-12\u0027]\n\n\n# country values\nSELECTED_COUNTRIES \u003d [\n    \u0027south africa\u0027,\n    \u0027kenya\u0027,\n    \u0027china\u0027,\n    \u0027taiwan\u0027,\n    \u0027japan\u0027,\n    \u0027south korea\u0027,\n    \u0027india\u0027,\n    \u0027pakistan\u0027,\n    \u0027indonesia\u0027,\n    \u0027philippines\u0027,\n    \u0027singapore\u0027,\n    \u0027thailand\u0027,\n    \u0027canada\u0027,\n    \u0027united kingdom\u0027,\n    \u0027ireland\u0027,\n    \u0027scotland\u0027,\n    \u0027australia\u0027,\n    \u0027new zealand\u0027,\n    \u0027united states\u0027,\n]\n\n\nCOUNTRIES_DICT \u003d {\n    \u0027australia\u0027 : \u0027australia\u0027,\n    \u0027canada\u0027 : \u0027canada\u0027,\n    \u0027china\u0027 : \u0027asia/china\u0027,\n    \u0027india\u0027 : \u0027asia/india\u0027,\n    \u0027indonesia\u0027 : \u0027asia/southeast/indonesia\u0027,\n    \u0027ireland\u0027 : \u0027europe/ireland\u0027,\n    \u0027japan\u0027 : \u0027asia/japan\u0027,\n    \u0027kenya\u0027 : \u0027africa/kenya\u0027,\n    \u0027new zealand\u0027 : \u0027new_zealand\u0027,\n    \u0027pakistan\u0027 : \u0027asia/pakistan\u0027,\n    \u0027philippines\u0027 : \u0027asia/philippines\u0027,\n    \u0027scotland\u0027 : \u0027europe/scotland\u0027,\n    \u0027singapore\u0027 : \u0027asia/singapore\u0027,\n    \u0027south africa\u0027 : \u0027africa/south_africa\u0027,\n    \u0027south korea\u0027 : \u0027asia/south_korea\u0027,\n    \u0027taiwan\u0027 : \u0027asia/taiwan\u0027,\n    \u0027thailand\u0027 : \u0027asia/thailand\u0027,\n    \u0027united kingdom\u0027 : \u0027europe/uk\u0027,\n    \u0027united states\u0027 : \u0027us\u0027\n}\n\n\n# News URL\nURL \u003d \u0027https://newslookup.com/{}?\u0026ut\u003d{}\u0026l\u003d1\u0026utto\u003d{}\u0027\nFOLDS \u003d 5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Headline Sentiment Data\n",
        "Headlines were collected for a set of countries by year. In order to use these headlines in further analysis, we want to calculate each headline\u0027s polarity and subjectivity and determine a mean for the year.\n",
        "\n",
        "Headlines are organized in a csv file with the following column headers:\n",
        "\n",
        "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
        "\n",
        "Together, the year and country columns are used as the index for the data.\n",
        "The analysis below creates three output dataframes, one for each headline\u0027s polarity, one for each headline\u0027s subjectivity, and one with average values of polarity and subjectivity for each year, per country. The headers of each of these are listed below.\n",
        "\n",
        "polarity_df and subjectivity_df:\n",
        "\n",
        "    | year | country | news0 | news1 | news2 | ... | news199 |\n",
        "\n",
        "average_sentiment_df:\n",
        "\n",
        "    | year | country | polarity | subjectivity |\n",
        "\n",
        "Both of the dataframes are indexed by year and country.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Read in the parsed news headlines from the csv file\n",
        "file_path \u003d os.path.join(DIR_NEWS, FILENAME_NEWS_HEADLINES)\n",
        "headlines_df \u003d pd.read_csv(file_path, sep\u003d\u0027|\u0027, index_col\u003d(0, 1))\n",
        "\n",
        "average_sentiment_columns \u003d [COL_YEAR, COL_COUNTRY, COL_POLARITY, COL_SUBJECTIVITY]\n",
        "average_sentiment_df \u003d pd.DataFrame(columns\u003daverage_sentiment_columns)\n",
        "\n",
        "news_columns \u003d [COL_NEWS_TEMPLATE.format(i) for i in range(200)]\n",
        "news_columns.insert(0, COL_COUNTRY)\n",
        "news_columns.insert(0, COL_YEAR)\n",
        "polarity_df \u003d pd.DataFrame(columns\u003dnews_columns)\n",
        "subjectivity_df \u003d pd.DataFrame(columns\u003dnews_columns)\n",
        "\n",
        "# iterate through the headline rows\n",
        "for index, row in headlines_df.iterrows():\n",
        "    # lists to store the individual values for each headline\n",
        "    polarity_list \u003d list()\n",
        "    subjectivity_list \u003d list()\n",
        "\n",
        "    polarity_list.extend([index[0], index[1]])\n",
        "    subjectivity_list.extend([index[0], index[1]])\n",
        "\n",
        "    # values for the avgerage yearly polarity and subjectivity\n",
        "    yearly_average_polarity \u003d 0\n",
        "    yearly_average_subjectivity \u003d 0\n",
        "    yearly_average_count \u003d 0\n",
        "\n",
        "    # calculate polarity and subjectivity for each headline\n",
        "    for entry in row:\n",
        "        if type(entry) \u003d\u003d float:\n",
        "            polarity_list.append(entry)\n",
        "            subjectivity_list.append(entry)\n",
        "\n",
        "        else:\n",
        "            blob \u003d TextBlob(entry)\n",
        "\n",
        "            pol_val \u003d 0\n",
        "            sub_val \u003d 0\n",
        "            count \u003d 0\n",
        "\n",
        "            # average the values in case a headline is multiple sentences\n",
        "            for sentence in blob.sentences:\n",
        "                pol_val \u003d pol_val + sentence.sentiment.polarity\n",
        "                sub_val \u003d sub_val + sentence.sentiment.subjectivity\n",
        "                count \u003d count + 1\n",
        "\n",
        "            polarity_list.append(pol_val / count)\n",
        "            subjectivity_list.append(sub_val / count)\n",
        "\n",
        "            yearly_average_polarity \u003d yearly_average_polarity + pol_val / count\n",
        "            yearly_average_subjectivity \u003d yearly_average_subjectivity + sub_val / count\n",
        "            yearly_average_count \u003d yearly_average_count + 1\n",
        "\n",
        "    yearly_average_polarity \u003d yearly_average_polarity / yearly_average_count\n",
        "    yearly_average_subjectivity \u003d yearly_average_subjectivity / yearly_average_count\n",
        "\n",
        "    yearly_average_df \u003d pd.DataFrame([[index[0], index[1], yearly_average_polarity, yearly_average_subjectivity]], columns\u003daverage_sentiment_columns)\n",
        "\n",
        "    pol_row_df \u003d pd.DataFrame([polarity_list], columns\u003dnews_columns)\n",
        "    sub_row_df \u003d pd.DataFrame([subjectivity_list], columns\u003dnews_columns)\n",
        "\n",
        "    average_sentiment_df \u003d pd.concat([average_sentiment_df, yearly_average_df], sort\u003dFalse)\n",
        "    polarity_df \u003d pd.concat([polarity_df, pol_row_df], sort\u003dFalse)\n",
        "    subjectivity_df \u003d pd.concat([subjectivity_df, sub_row_df], sort\u003dFalse)\n",
        "\n",
        "# these are all of the polarities and subjectivities for each headline\n",
        "polarity_df \u003d polarity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
        "subjectivity_df \u003d subjectivity_df.set_index([COL_YEAR, COL_COUNTRY])\n",
        "average_sentiment_df \u003d average_sentiment_df.set_index([COL_YEAR, COL_COUNTRY])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def load_data_frame(file_name, path, sep\u003dNone):\n",
        "    \"\"\"\n",
        "    Loads data from specified path and name, returns a dataframe\n",
        "    \"\"\"\n",
        "    file_path \u003d os.path.join(path, file_name)\n",
        "    if not sep:\n",
        "        return pd.read_csv(file_path)\n",
        "    return pd.read_csv(file_path, sep\u003dsep)\n",
        "\n",
        "\n",
        "def load_mental_health_data():\n",
        "    \"\"\"\n",
        "    Loads mental health data, and performs basic preprocessing operations:\n",
        "        - columns renamed appropriately for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "    \"\"\"\n",
        "    mental_df \u003d load_data_frame(\n",
        "        FILENAME_DISORDERS,\n",
        "        DIR_MENTAL_HEALTH\n",
        "    )\n",
        "\n",
        "    mental_df.rename(columns\u003d{\u0027Entity\u0027: COL_COUNTRY, \u0027Year\u0027: COL_YEAR}, inplace\u003dTrue)\n",
        "    mental_df.drop(labels\u003d\u0027Code\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "    mental_df[COL_COUNTRY] \u003d mental_df[COL_COUNTRY].str.lower()\n",
        "    mental_df \u003d mental_df[mental_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "    return mental_df\n",
        "\n",
        "\n",
        "def load_world_bank_data(filename, directory, value):\n",
        "    \"\"\"\n",
        "    Loads World Bank data, and performs basic preprocessing operations:\n",
        "        - columns renamed appropriately for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "        - non-numerical values are replaced with NaN or transformed appropriately\n",
        "    \"\"\"\n",
        "    df \u003d load_data_frame(\n",
        "        filename,\n",
        "        directory\n",
        "    )\n",
        "    df.rename(columns\u003d{\u0027Country Name\u0027: COL_COUNTRY}, inplace\u003dTrue)\n",
        "    df.drop([\u0027Indicator Name\u0027, \u0027Indicator Code\u0027, \u0027Country Code\u0027], axis\u003d1, inplace \u003d True)\n",
        "    df \u003d df.replace(\u0027..\u0027, np.NaN)\n",
        "    df.loc[:,1:] \u003d df.iloc[:, 1:].apply(pd.to_numeric)\n",
        "    df[COL_COUNTRY] \u003d df[COL_COUNTRY].str.lower()\n",
        "    df \u003d df.replace(\u0027korea, rep.\u0027, \u0027south korea\u0027)\n",
        "    df \u003d df.loc[df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "\n",
        "    columns \u003d df.columns\n",
        "    country_index \u003d columns.get_loc(COL_COUNTRY)\n",
        "    start_index \u003d columns.get_loc(DATE_START)\n",
        "    end_index \u003d columns.get_loc(DATE_END)\n",
        "    df \u003d df.iloc[:, np.r_[country_index, start_index:end_index]]\n",
        "\n",
        "    # Reshape the dataframe to structure `country|year|value`\n",
        "    df \u003d pd.melt(\n",
        "        df,\n",
        "        id_vars\u003dCOL_COUNTRY,\n",
        "        var_name\u003dCOL_YEAR,\n",
        "        value_name\u003dvalue\n",
        "    )\n",
        "    df[COL_YEAR] \u003d df[COL_YEAR].apply(pd.to_numeric)\n",
        "    df \u003d df.sort_values([COL_COUNTRY, COL_YEAR])\n",
        "    df.reset_index(inplace\u003dTrue, drop\u003dTrue)\n",
        "    df.dropna(axis\u003d0, inplace\u003dTrue)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_hdi_data():\n",
        "    \"\"\"\n",
        "    Loads human development index (HDI) data, and performs basic preprocessing operations:\n",
        "        - columns are renamed for compatibility\n",
        "        - selected countries are filtered\n",
        "        - unnecessary columns are dropped\n",
        "    \"\"\"\n",
        "    hdi_df \u003d load_data_frame(\n",
        "        FILENAME_HDI,\n",
        "        DIR_HDI\n",
        "    )\n",
        "\n",
        "    hdi_df \u003d hdi_df.dropna(how\u003d\u0027all\u0027, axis\u003d1)\n",
        "    hdi_df.drop(\u0027HDI Rank (2017)\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "    hdi_df.rename(columns\u003d{\u0027Country\u0027: COL_COUNTRY, \u0027Year\u0027: COL_YEAR}, inplace\u003dTrue)\n",
        "    hdi_df[COL_COUNTRY] \u003d hdi_df[COL_COUNTRY].str.lower().str.strip()\n",
        "    hdi_df \u003d hdi_df[hdi_df[COL_COUNTRY].isin(SELECTED_COUNTRIES)]\n",
        "    hdi_df \u003d hdi_df.reset_index(drop\u003dTrue)\n",
        "    hdi_df.loc[:,1:] \u003d hdi_df.iloc[:,1:].apply(pd.to_numeric)\n",
        "\n",
        "    # Reshape the dataframe to structure `country|year|value`\n",
        "    hdi_df \u003d pd.melt(\n",
        "        hdi_df,\n",
        "        id_vars\u003d[COL_COUNTRY],\n",
        "        var_name\u003dCOL_YEAR,\n",
        "        value_name\u003dCOL_HDI\n",
        "    )\n",
        "\n",
        "    hdi_df[COL_YEAR] \u003d hdi_df[COL_YEAR].apply(pd.to_numeric)\n",
        "    hdi_df \u003d hdi_df.drop(hdi_df[hdi_df.year \u003c int(DATE_START)].index)\n",
        "    hdi_df \u003d hdi_df.drop(hdi_df[hdi_df.year \u003e int(DATE_END)].index)\n",
        "    return hdi_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Joining Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "population_df \u003d load_world_bank_data(FILENAME_POPULATION_DENSITY, DIR_POPULATION, COL_POPULATION_DENSITY)\n",
        "urban_df \u003d load_world_bank_data(FILENAME_URBAN_DENSITY, DIR_POPULATION, COL_URBAN_DENSITY)\n",
        "gdp_df \u003d load_world_bank_data(FILENAME_GDP, DIR_FINANCIAL, COL_GDP)\n",
        "unemployment_df \u003d load_world_bank_data(FILENAME_UNEMPLOYMENT, DIR_FINANCIAL, COL_UNEMPLOYMENT)\n",
        "hdi_df \u003d load_hdi_data()\n",
        "mental_df \u003d load_mental_health_data()\n",
        "\n",
        "\n",
        "\n",
        "# Joining individual datasets on population and urban population density, \n",
        "# GDP, unemployment, HDI, average news headlines sentiment, and mental health data\n",
        "data_frames_list \u003d [\n",
        "    population_df,\n",
        "    urban_df,\n",
        "    gdp_df,\n",
        "    unemployment_df,\n",
        "    hdi_df,\n",
        "    average_sentiment_df, \n",
        "    mental_df\n",
        "]\n",
        "\n",
        "joined_df \u003d reduce(lambda left, right: pd.merge(left, right, on\u003d[COL_COUNTRY, COL_YEAR], how\u003d\u0027inner\u0027), data_frames_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Pipeline Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "# Regressors and their tuning parameters\nnet \u003d MLPRegressor()\nnet_parameters \u003d {\n    \u0027net__learning_rate\u0027: [\u0027constant\u0027, \u0027invscaling\u0027, \u0027adaptive\u0027],\n    \u0027net__hidden_layer_sizes\u0027: [(50), (100), (50, 50), (100, 100), (150, 150), (100, 100, 100)],\n    \u0027net__max_iter\u0027: [10, 100, 1000]\n}\n\nlinear \u003d LinearRegression()\nlinear_parameters \u003d {\n    \u0027linear__fit_intercept\u0027: [True, False]\n}\n\nforest \u003d RandomForestRegressor()\nforest_parameters \u003d {\n    \u0027forest__n_estimators\u0027: [10, 20, 50],\n    \u0027forest__max_depth\u0027: [2, 5],\n    \u0027forest__min_samples_split\u0027: [3, 4, 5],\n    \u0027forest__max_features\u0027: [\u0027auto\u0027, \u0027sqrt\u0027, \u0027log2\u0027]\n}\n\nstochastic \u003d SGDRegressor()\nstochastic_parameters \u003d {\n    \u0027stochastic__alpha\u0027: [0.0001, 0.001, 0.01, 0.1],\n    \u0027stochastic__penalty\u0027: [\u0027l2\u0027,\u0027l1\u0027],\n    \u0027stochastic__learning_rate\u0027: [\u0027invscaling\u0027, \u0027optimal\u0027, \u0027constant\u0027, \u0027adaptive\u0027]\n}\n\nknn \u003d KNeighborsRegressor()\nknn_parameters \u003d {\n    \u0027knn__n_neighbors\u0027: [5, 10, 20, 30]\n}\n\n# Scalers\nscaler \u003d MinMaxScaler()\n\n# Feature Selectors\nfeature_selector \u003d SelectKBest()\nselector_parameters \u003d {\n    \u0027selector__k\u0027: range(1, len(FEATURES)+1)\n}\n\n# Pipelines\nnet_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027net\u0027, net)])\nlinear_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027linear\u0027, linear)])\nforest_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027forest\u0027, forest)])\nstochastic_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027stochastic\u0027, stochastic)])\nknn_pipeline \u003d Pipeline(steps\u003d[(\u0027scaler\u0027, scaler), (\u0027selector\u0027, feature_selector), (\u0027knn\u0027, knn)])\n\n\ndef get_grid_search(pipeline, parameters):\n    return GridSearchCV(pipeline, parameters, cv\u003dFOLDS, n_jobs\u003d-1, verbose\u003d5)\n    \n\ngrid_searcher_net \u003d get_grid_search(net_pipeline, dict(net_parameters, **selector_parameters))\ngrid_searcher_linear \u003d get_grid_search(linear_pipeline, dict(selector_parameters, **linear_parameters))\ngrid_searcher_forest \u003d get_grid_search(forest_pipeline, dict(selector_parameters, **forest_parameters))\ngrid_searcher_stochastic \u003d get_grid_search(stochastic_pipeline, dict(selector_parameters, **stochastic_parameters))\ngrid_searcher_knn \u003d get_grid_search(knn_pipeline, dict(selector_parameters, **knn_parameters))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index([\u0027country\u0027, \u0027year\u0027, \u0027population density\u0027, \u0027urban density (%)\u0027, \u0027GDP\u0027,\n       \u0027unemployment rate\u0027, \u0027HDI\u0027, \u0027polarity\u0027, \u0027subjectivity\u0027,\n       \u0027Schizophrenia (%)\u0027, \u0027Bipolar disorder (%)\u0027, \u0027Eating disorders (%)\u0027,\n       \u0027Anxiety disorders (%)\u0027, \u0027Drug use disorders (%)\u0027, \u0027Depression (%)\u0027,\n       \u0027Alcohol use disorders (%)\u0027],\n      dtype\u003d\u0027object\u0027)"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 123
        }
      ],
      "source": [
        "joined_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cb\u003eAnxiety Disorders\u003c/b\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Columns in Training Data\n Index([\u0027GDP\u0027, \u0027HDI\u0027, \u0027polarity\u0027, \u0027population density\u0027, \u0027subjectivity\u0027,\n       \u0027unemployment rate\u0027, \u0027urban density (%)\u0027],\n      dtype\u003d\u0027object\u0027)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Removing indexing columns (country, year) and rates that will not be predicted\nTARGET \u003d \u0027Anxiety disorders (%)\u0027\n\ny \u003d joined_df.copy().pop(TARGET)\nx \u003d joined_df.copy()[FEATURES]\n\nx_train, x_test, y_train, y_test \u003d train_test_split(x, y, random_state\u003d0)\nprint(\u0027Columns in Training Data\\n\u0027, x.columns)"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n",
            "\t****\tNEURAL NET REGRESSOR\t****\nBest Train score: 0.7373697172593103\nTest score: 0.8469490356059316\nBest estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d4, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027net\u0027, MLPRegressor(activation\u003d\u0027relu\u0027, alpha\u003d0.0001, batch_size\u003d\u0027auto\u0027, beta_1\u003d0.9,\n       beta_2\u003d0.999, early_stopping\u003dFalse, epsilon\u003d1e-08,\n       hidd...\u003dTrue, solver\u003d\u0027adam\u0027, tol\u003d0.0001,\n       validation_fraction\u003d0.1, verbose\u003dFalse, warm_start\u003dFalse))])\nBest parameters: {\u0027net__hidden_layer_sizes\u0027: (50, 50), \u0027net__learning_rate\u0027: \u0027adaptive\u0027, \u0027net__max_iter\u0027: 1000, \u0027selector__k\u0027: 4}\nFeatures used: [\u0027GDP\u0027 \u0027HDI\u0027 \u0027polarity\u0027 \u0027urban density (%)\u0027]\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  10 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 110 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 254 tasks      | elapsed:   24.7s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 537 tasks      | elapsed:   53.0s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 834 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1226 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1694 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1890 out of 1890 | elapsed:  2.2min finished\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "grid_searcher_net.fit(x_train, y_train)\nprint(\u0027\\t****\\tNEURAL NET REGRESSOR\\t****\u0027)\nprint(\u0027Best Train score:\u0027, grid_searcher_net.best_score_)\nprint(\u0027Test score:\u0027, grid_searcher_net.best_estimator_.score(x_test, y_test))\nprint(\u0027Best estimator:\u0027, grid_searcher_net.best_estimator_)\nprint(\u0027Best parameters:\u0027, grid_searcher_net.best_params_)\n\nnum_features_used \u003d grid_searcher_net.best_params_[\u0027selector__k\u0027]\nif num_features_used \u003d\u003d \u0027all\u0027:\n    num_features_used \u003d len(FEATURES)\nelse:\n    num_features_used \u003d int(num_features_used)\nscores \u003d grid_searcher_net.best_estimator_.named_steps.selector.scores_\nhighest_scores_indices \u003d np.argpartition(scores, -num_features_used)[-num_features_used:]\n\nfeatures_used \u003d x_train.columns[highest_scores_indices].values\nprint(\u0027Features used:\u0027, features_used)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
            "\t****\tLINEAR REGRESSOR\t****\nBest Train score: 0.7039916938507242\nTest score: 0.8623185517949138\nBest estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d7, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027linear\u0027, LinearRegression(copy_X\u003dTrue, fit_intercept\u003dTrue, n_jobs\u003dNone,\n         normalize\u003dFalse))])\nBest parameters: {\u0027linear__fit_intercept\u0027: True, \u0027selector__k\u0027: 7}\nFeatures used: [\u0027GDP\u0027 \u0027HDI\u0027 \u0027polarity\u0027 \u0027population density\u0027 \u0027subjectivity\u0027\n \u0027unemployment rate\u0027 \u0027urban density (%)\u0027]\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  12 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs\u003d-1)]: Done  70 out of  70 | elapsed:    0.9s finished\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "grid_searcher_linear.fit(x_train, y_train)\nprint(\u0027\\t****\\tLINEAR REGRESSOR\\t****\u0027)\nprint(\u0027Best Train score:\u0027, grid_searcher_linear.best_score_)\nprint(\u0027Test score:\u0027, grid_searcher_linear.best_estimator_.score(x_test, y_test))\nprint(\u0027Best estimator:\u0027, grid_searcher_linear.best_estimator_)\nprint(\u0027Best parameters:\u0027, grid_searcher_linear.best_params_)\n\n\nnum_features_used \u003d grid_searcher_linear.best_params_[\u0027selector__k\u0027]\nif num_features_used \u003d\u003d \u0027all\u0027:\n    num_features_used \u003d len(FEATURES)\nelse:\n    num_features_used \u003d int(num_features_used)\nscores \u003d grid_searcher_linear.best_estimator_.named_steps.selector.scores_\nhighest_scores_indices \u003d np.argpartition(scores, -num_features_used)[-num_features_used:]\n\nfeatures_used \u003d x_train.columns[highest_scores_indices].values\nprint(\u0027Features used:\u0027, features_used)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  12 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 120 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 300 tasks      | elapsed:    6.0s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 552 tasks      | elapsed:   10.8s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 876 tasks      | elapsed:   16.9s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1272 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1740 tasks      | elapsed:   33.9s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1890 out of 1890 | elapsed:   37.1s finished\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n",
            "\t****\tRANDOM FOREST REGRESSOR\t****\nBest Train score: 0.958234265287273\nTest score: 0.9647322800946572\nBest estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d6, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027forest\u0027, RandomForestRegressor(bootstrap\u003dTrue, criterion\u003d\u0027mse\u0027, max_depth\u003d5,\n           max_features\u003d\u0027auto\u0027, max_leaf_nodes\u003dNone,\n           min_impurit...ators\u003d10, n_jobs\u003dNone,\n           oob_score\u003dFalse, random_state\u003dNone, verbose\u003d0, warm_start\u003dFalse))])\nBest parameters: {\u0027forest__max_depth\u0027: 5, \u0027forest__max_features\u0027: \u0027auto\u0027, \u0027forest__min_samples_split\u0027: 4, \u0027forest__n_estimators\u0027: 10, \u0027selector__k\u0027: 6}\nFeatures used: [\u0027HDI\u0027 \u0027polarity\u0027 \u0027population density\u0027 \u0027subjectivity\u0027 \u0027unemployment rate\u0027\n \u0027urban density (%)\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "grid_searcher_forest.fit(x_train, y_train)\nprint(\u0027\\t****\\tRANDOM FOREST REGRESSOR\\t****\u0027)\nprint(\u0027Best Train score:\u0027, grid_searcher_forest.best_score_)\nprint(\u0027Test score:\u0027, grid_searcher_forest.best_estimator_.score(x_test, y_test))\nprint(\u0027Best estimator:\u0027, grid_searcher_forest.best_estimator_)\nprint(\u0027Best parameters:\u0027, grid_searcher_forest.best_params_)\n\n\nnum_features_used \u003d grid_searcher_forest.best_params_[\u0027selector__k\u0027]\nif num_features_used \u003d\u003d \u0027all\u0027:\n    num_features_used \u003d len(FEATURES)\nelse:\n    num_features_used \u003d int(num_features_used)\nscores \u003d grid_searcher_forest.best_estimator_.named_steps.selector.scores_\nhighest_scores_indices \u003d np.argpartition(scores, -num_features_used)[-num_features_used:]\n\nfeatures_used \u003d x_train.columns[highest_scores_indices].values\nprint(\u0027Features used:\u0027, features_used)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  16 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 232 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 592 tasks      | elapsed:    5.1s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 1120 out of 1120 | elapsed:    9.3s finished\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n",
            "\t****\tSTOCHASTIC GRADIENT DESCENT REGRESSOR\t****\nBest Train score: 0.4316804137404207\nTest score: 0.5673357679304032\nBest estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d7, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027stochastic\u0027, SGDRegressor(alpha\u003d0.1, average\u003dFalse, early_stopping\u003dFalse, epsilon\u003d0.1,\n       eta0\u003d0.01, fit_intercept\u003dTrue, l1_ratio\u003d0.15,\n       learn...m_state\u003dNone, shuffle\u003dTrue, tol\u003dNone, validation_fraction\u003d0.1,\n       verbose\u003d0, warm_start\u003dFalse))])\nBest parameters: {\u0027selector__k\u0027: 7, \u0027stochastic__alpha\u0027: 0.1, \u0027stochastic__learning_rate\u0027: \u0027constant\u0027, \u0027stochastic__penalty\u0027: \u0027l2\u0027}\nFeatures used: [\u0027GDP\u0027 \u0027HDI\u0027 \u0027polarity\u0027 \u0027population density\u0027 \u0027subjectivity\u0027\n \u0027unemployment rate\u0027 \u0027urban density (%)\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "grid_searcher_stochastic.fit(x_train, y_train)\nprint(\u0027\\t****\\tSTOCHASTIC GRADIENT DESCENT REGRESSOR\\t****\u0027)\nprint(\u0027Best Train score:\u0027, grid_searcher_stochastic.best_score_)\nprint(\u0027Test score:\u0027, grid_searcher_stochastic.best_estimator_.score(x_test, y_test))\nprint(\u0027Best estimator:\u0027, grid_searcher_stochastic.best_estimator_)\nprint(\u0027Best parameters:\u0027, grid_searcher_stochastic.best_params_)\n\n\nnum_features_used \u003d grid_searcher_stochastic.best_params_[\u0027selector__k\u0027]\nif num_features_used \u003d\u003d \u0027all\u0027:\n    num_features_used \u003d len(FEATURES)\nelse:\n    num_features_used \u003d int(num_features_used)\nscores \u003d grid_searcher_stochastic.best_estimator_.named_steps.selector.scores_\nhighest_scores_indices \u003d np.argpartition(scores, -num_features_used)[-num_features_used:]\n\nfeatures_used \u003d x_train.columns[highest_scores_indices].values\nprint(\u0027Features used:\u0027, features_used)\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs\u003d-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs\u003d-1)]: Done  20 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 104 out of 140 | elapsed:    1.2s remaining:    0.4s\n",
            "[Parallel(n_jobs\u003d-1)]: Done 140 out of 140 | elapsed:    1.5s finished\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
            "\t****\tK-NEAREST NEIGHBORS REGRESSOR\t****\nBest Train score: 0.6760881468825559\nTest score: 0.897100082257263\nBest estimator: Pipeline(memory\u003dNone,\n     steps\u003d[(\u0027scaler\u0027, MinMaxScaler(copy\u003dTrue, feature_range\u003d(0, 1))), (\u0027selector\u0027, SelectKBest(k\u003d7, score_func\u003d\u003cfunction f_classif at 0x11bf967b8\u003e)), (\u0027knn\u0027, KNeighborsRegressor(algorithm\u003d\u0027auto\u0027, leaf_size\u003d30, metric\u003d\u0027minkowski\u0027,\n          metric_params\u003dNone, n_jobs\u003dNone, n_neighbors\u003d5, p\u003d2,\n          weights\u003d\u0027uniform\u0027))])\nBest parameters: {\u0027knn__n_neighbors\u0027: 5, \u0027selector__k\u0027: 7}\nFeatures used: [\u0027GDP\u0027 \u0027HDI\u0027 \u0027polarity\u0027 \u0027population density\u0027 \u0027subjectivity\u0027\n \u0027unemployment rate\u0027 \u0027urban density (%)\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "grid_searcher_knn.fit(x_train, y_train)\nprint(\u0027\\t****\\tK-NEAREST NEIGHBORS REGRESSOR\\t****\u0027)\nprint(\u0027Best Train score:\u0027, grid_searcher_knn.best_score_)\nprint(\u0027Test score:\u0027, grid_searcher_knn.best_estimator_.score(x_test, y_test))\nprint(\u0027Best estimator:\u0027, grid_searcher_knn.best_estimator_)\nprint(\u0027Best parameters:\u0027, grid_searcher_knn.best_params_)\n\n\nnum_features_used \u003d grid_searcher_knn.best_params_[\u0027selector__k\u0027]\nif num_features_used \u003d\u003d \u0027all\u0027:\n    num_features_used \u003d len(FEATURES)\nelse:\n    num_features_used \u003d int(num_features_used)\nscores \u003d grid_searcher_knn.best_estimator_.named_steps.selector.scores_\nhighest_scores_indices \u003d np.argpartition(scores, -num_features_used)[-num_features_used:]\n\nfeatures_used \u003d x_train.columns[highest_scores_indices].values\nprint(\u0027Features used:\u0027, features_used)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}